<!DOCTYPE HTML>
<html xml:lang="en" lang="en">

<head>
    <title>CSE490G1 | Final Project</title>
    <link rel="stylesheet" href="./src/stylesheets/style.css">
    <script src="https://cdn.jsdelivr.net/npm/arquero@latest"></script>
    <script src="https://cdn.jsdelivr.net/npm/vega@latest"></script> 
    <script src="https://cdn.jsdelivr.net/npm/vega-lite@latest"></script>
    <script src="https://cdn.jsdelivr.net/npm/vega-lite-api@latest"></script>
    <script src="https://cdn.jsdelivr.net/npm/vega-tooltip@latest"></script>
</head>

<body>
    <section class="title">
        <h1>Pruning CNNs</h1>
        <h3>Harrison Bay &nbsp;&nbsp;&nbsp; Arnav Das</h3>
        <p><a href="https://github.com/harrisonbay/CNNPruning" style="font-size: 20px">Source Code</a> | <a href="#results" style="font-size: 20px">Results</a>
        </p>
    </section>
    <section class="diagram">
        <div class="nncontainer">
            <img class="nn" src="assets/images/unpruned.svg">
        </div>
        <div class="nncontainer" style="padding-top:20px; padding-bottom:20px;">
            <img src="assets/images/arrow.svg">
            <text>50% prune</text>
        </div>
        <div class="nncontainer">
            <img class="nn" src="assets/images/pruned.svg">
        </div>
    </section>

    <hr>

    <iframe width="560" height="315" src="https://www.youtube.com/embed/0fu1qMV1HWo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

    <section class="introduction text">
        <h2>Introduction</h2>
        <p> Since the early 2010's, it is has become increasingly apparent that convolutional neural networks are the model of choice for most vision tasks, consistently achieving
        state of the art performance on image dataset benchmarks. Furthermore, these models learn representations that can be easily transferred; we can use a pretrained 
        model and fine tune it on a simpler target dataset and still achieve strong performance. However, most neural networks are overparameterized which can lead to high latency at inference time, making them suboptimal for deployment on real time systems (e.g. autonomous cars).</p>

        <p>Therefore, for our final project we aim to compress a VGG-19 network with batch normalization that has been pretrained on ImageNet and then fine tune it on the German Traffic Sign Dataset. Many convolutional neural networks consist of two distinct parts: a feature extractor, with many convolutional layers that are computationally intense, and a classification layer, with a few dense, fully-connected layers that are memory intense. We focus on the feature extractor and identify and test methods that allow us to remove a large number of filters from the model while still ensuring that the resulting pruned model can still effectively learn the target dataset. In the scope of this project, we implement several different pruning methods and evaluate the effectiveness of each approach based on how well the pruned model generalizes on the target dataset. <p>
    </section>

    <section class="related-work text">
        <h2>Related Work</h2>

        <p>In recent years, there has been much interest in the area of neural network compression. The seminal paper of "Deep Compression: Compressing Deep Neural Networks With Pruning, Trained Quantization and Huffman Coding" explored several different methods of neural network compression (<a href="https://arxiv.org/pdf/1510.00149.pdf">Han et al., 2016</a>). In particular, the paper focuses on methods that compress based on  individual weights: they propose pruning by simply zeroing out weights below a certain magnitude, and weight quantization "bins" individual weights to discrete values. In contrast, our project focuses on pruning entire convolutional filters.</p>

        <p>Two of the pruning methods we tested in our project include rank-based pruning and gradient-based pruning introduced by "HRank: Filter Pruning using High-Rank Feature Map" (<a href="https://arxiv.org/pdf/2002.10179.pdf">Lin et al., 2020</a>) and "Pruning Convolutional Neural Networks for Resource Efficient Inference" (<a href="https://arxiv.org/pdf/1611.06440.pdf">Molchanov et al., 2017</a>), respectively. We repurpose the approaches and analyze their efficacy based on the performance on the downstream transfer learning task described above. The approaches, along with their underlying assumptions, are discussed in detail below.</p>
    </section>

    <section class="data text">
        <h2>Data</h2>
        <figure>
          <img src="assets/images/samples.png" style="height:400px; width:400px;">
        </figure>
        <h5 style="text-align:center;">Sample processed training images</h5>
        <p>For the target dataset, we use a portion of the <a href="https://www.kaggle.com/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign">German Traffic Sign Recognition Benchmark</a>  which consists of roughly 53000 images of traffic signs (40000 training and 13000 validation). Training was done on Google Colab, so we had to figure out how to get our data onto Google Drive. We initially uploaded all of the training images to Google Drive with <a href="https://rclone.org/">rclone</a>. However, this was not a good solution since there is a lot of latency when using <span style="font-family: menlo;">imread</span> to read in images through Google Drive. As an alternative solution, we pre-processed the data locally with <a href="https://www.h5py.org/">h5py</a> and <a href="https://scikit-image.org/">skimage</a> and applied some transformations, namely: 
            <ol>
                <li>Cropped the image with the bounding box labels provided;</li>
                <li>Resized the image to 32 by 32 pixels;</li>
                <li>Performed contrast-limited adaptive histogram equalization (CLAHE) on the images.</li>
            </ol>
        The pre-processing script is available with the source code. We show (CLAHE-d) example images of the dataset above.</p>
    </section>

    <section class="approach text">
        <h2>Approach</h2>
        <figure>
          <img src="assets/images/filters.png" style="height:499px; width:1000px;">
        </figure>
        <h5 style="text-align:center;">0.25 pruning ratio performed by k-means clustering (64->16 filters)</h5>
        <p>For our experiments, we obtain a VGG-19 network with batch normalization that has been pretrained on ImageNet, prune a certain percentage of the filters from each convolutional layer, then fine-tune on the target dataset for three epochs with stochastic gradient descent with a learning rate lr = 1e-3 (other hyperparameters available in the code). We discuss all of the candidate pruning methods and categorize them based on their assumptions below.</p>

        <h3>Data-Free Pruning Methods</h3>
        <h4>K-means clustering</h4>
        <p>In overparameterized models, there are typically many redundant filters that extract similar features. Therefore, a natural pruning objective is to maximize filter diversity in each convolutional layer. Therefore, if we want to retain n filters in a given layer, we apply k-means clustering to recover n centroids, and select the n filters that are closest to the centroids. We use Euclidean distance to quantify the distance between two filters. Note that this method applies clustering on the <i>filters</i>, not the activations, so this method is independent of the target dataset distribution.</p>

        <h3>Label-Free Pruning Methods</h3>
        <h4>L2-norm</h4>
        <p>Intuitively, filters that produce low magnitude activations are probably less important for the task and should be removable without substantial loss of accuracy. Based on this notion, we generate a small subset of data (n = 512) from the target dataset, pass it through the model, recover the activations of each layer, and retain the k filters whose feature maps have the highest L2-norms. This method relies on the target data, but does not require them to be labeled.</p>

        <h4>"HRank" (high-rank)</h4>
        <p>The "HRank" method, suggests using a slightly more sophisticated ranking criteria than the L2-norm of the feature maps. <a href="https://arxiv.org/pdf/1611.06440.pdf">Molchanov et al., 2017</a> argue that the rank of a feature map is a much more rich information measure that is more likely to be indicative of a filters importance. This method is identical to the previous method, but instead of retaining the filters with the highest magnitudes, we retain the filters whose feature maps have the highest ranks. This method also requires us to feed a subset of the target dataset through the model.</p>

        <h3>Label-Dependent Pruning Methods</h3>
        <h4>Gradient-based</h4>
        <p>The final method that we implement is one that incorporates information about the gradient into our evaluation metric. Mathematically, the <b><i>saliency</i></b> of a particular weight in a filter can be expressed as the following:</p>
        <div class="displaymath">
            s = |w(∂L / ∂w)|
        </div>
        <p>To compute the saliency of the entire filter, we simply sum over the saliencies of each individual weight. Intuitively, the gradient captures the degree to which the output of the model depends on a particular weight and thus can lead to a more informed pruning criteria. However, this method requires a labeled subset of the target dataset.</p>
    </section>

    <section id="results" class="results text">
        <h2>Results</h2>
        <figure id="vis"></figure>
        <p>As we can see in the above chart, l2-norm pruning clearly performs the worst. This was expected as it was our baseline pruning method. The gradient-based method outperforms the HRank and l2-norm pruning methods, which was also consistent with our expectations. However, to our surprise, k-means clustering pruning appeared to perform about on par, or even a little bit better, than gradient pruning. This could suggest, more generally, that filter diversity is important to consider when formulating a pruning criteria. Hover over specific data points if you wish to look at the accuracies in detail.
    </section>

    <section class="discussion text" style="margin-bottom: 120px;">
        <h2>Discussion</h2>
        <p> There are many possible ways to improve upon our results. For starters, we apply a uniform pruning ratio to all convolutional layers in the network. However, it is entirely possible
            that better results are attainable if a heuristic is used to determine the number of filters to be pruned in a layer based on its depth. Furthermore, we only explore pruning filters and 
            do not apply any compression to the dense layers. While this is able to significantly reduce the number of floating point operations at inference time, we are hardly able to reduce memory usage. 
            At a more fundamental level, it is also important to ask whether it is better to train a large model and apply pruning afterwards or simply to train a smaller model using knowledge distillation techniques. <p>
            
        <p> Through the experiments carried out in this final project, we have successfully shown that the compuational costs of neural networks can significantly reduced through filter pruning. <p>
        
    </section>

    <footer id="about">
      <div class="about-container">
        <div class="about-text">
          <p>This page was created as the Final Project for CSE 490G1/599, au20 at the University of Washington.</p>
        </div>
      </div>
    </footer>

    <script src="src/js/viz.js"></script>
</body>

</html>


